# -*- coding: utf-8 -*-
"""Market Basket Analisys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BMrdSc0E4E_6P7bp_XwoL5M9ElGBKYQ-
"""

"""Market Basket Analisys menggunakan Python"""

# Import Libary
# Operasi Dasar
import numpy as np
import pandas as pd

# Visualisasi
import matplotlib.pyplot as plt
import squarify
import seaborn as sns
plt.style.use('fivethirtyeight')
import warnings
warnings.filterwarnings('ignore')

# Market Basket Analysis
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from mlxtend.preprocessing import TransactionEncoder
from wordcloud import WordCloud

# Membaca DataSet
data = pd.read_csv('/content/Market_Basket_Optimisation.csv', header = None)

# Melihat Ukuran Data
data.shape

# Melihat Beberapa Data yang Paling Atas
data.head()

# Visualisasi Menggunakan Wordcloud
plt.rcParams['figure.figsize'] = (15, 15)
wordcloud = WordCloud(background_color = 'white', width = 1200,  height = 1200, max_words = 121).generate(str(data[0]))
plt.imshow(wordcloud)
plt.axis('off')
plt.title('Most Popular Items',fontsize = 12)
plt.show()

# Melihat Frekuensi Item Paling Populer
plt.rcParams['figure.figsize'] = (18, 7)
color = plt.cm.copper(np.linspace(0, 1, 40))
data[0].value_counts().head(40).plot.bar(color = color)
plt.title('Frekuensi Item yang Populer', fontsize = 20)
plt.xticks(rotation = 90 )
plt.grid()
plt.show()

# Visualisasi Menggunakan Tree Map (Peta Pohon)
y = data[0].value_counts().head(50).to_frame()
y.index# plotting a tree map
plt.rcParams['figure.figsize'] = (20, 20)
color = plt.cm.cool(np.linspace(0, 1, 50))
squarify.plot(sizes = y.values, label = y.index, alpha=.8, color = color)
plt.title('Tree Map for Item Populer')
plt.axis('off')
plt.show()

# Pre-Processing Data
# membuat setiap barang belanjaan pelanggan menjadi daftar yang identik 
trans = []
for i in range(0, 7501):
    trans.append([str(data.values[i,j]) for j in range(0, 20)])

# mengubahnya menjadi array numpy 
trans = np.array(trans)

# Memeriksa Bentuknya
print(trans.shape)

# Transaction Encoder
te = TransactionEncoder()
data = te.fit_transform(trans)
data = pd.DataFrame(data, columns = te.columns_)
data.shape

# Mereduksi Kolom Hingga Menjadi 40
# getting correlations for 121 items would be messy 
# so let's reduce the items from 121 to 40

data = data.loc[:, ['mineral water', 'burgers', 'turkey', 'chocolate', 'frozen vegetables', 'spaghetti',
                    'shrimp', 'grated cheese', 'eggs', 'cookies', 'french fries', 'herb & pepper', 'ground beef',
                    'tomatoes', 'milk', 'escalope', 'fresh tuna', 'red wine', 'ham', 'cake', 'green tea',
                    'whole wheat pasta', 'pancakes', 'soup', 'muffins', 'energy bar', 'olive oil', 'champagne', 
                    'avocado', 'pepper', 'butter', 'parmesan cheese', 'whole wheat rice', 'low fat yogurt', 
                    'chicken', 'vegetables mix', 'pickles', 'meatballs', 'frozen smoothie', 'yogurt cake']]
data.shape

# Mengecek Kolom Setelah Direduksi
data.columns

# Menerapkan Algoritma Apriori
#Now, let us return the items and itemsets with at least 5% support:
apriori(data, min_support = 0.01, use_colnames = True)

frequent_itemsets = apriori(data, min_support = 0.05, use_colnames=True)
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))

# getting th item sets with length = 2 and support more than 1%
frequent_itemsets[ (frequent_itemsets['length'] == 2) &
                   (frequent_itemsets['support'] >= 0.01) ]

